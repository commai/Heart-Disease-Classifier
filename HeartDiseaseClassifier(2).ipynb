{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"#Predicting Heart Disease with Adaboost (Based off a decision tree classifier)\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing data\ndf = pd.read_csv('heart_disease.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handling the null values in the dataset\nfig = plt.figure(figsize=(15,15))\nax = fig.gca()\ndf.hist(ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CP  has missing values, need to input the data\n# The data is numeric, so using a mean strategy would be a good choice\n\nfrom sklearn.preprocessing import Imputer\nimput = Imputer(missing_values='NaN',strategy='mean')\ndf = list(imput.fit_transform(df))\n\nfor i in range(303):\n    for j in range(14):\n        df[i][j] = math.ceil(df[i][j])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.rename(columns={0:'age', 1:'sex', 2:'cp', 3:'trestbps',4: 'chol',5: 'fbs',6: 'restecg',7: 'thalach',8: 'exang',9: 'oldpeak',10: 'slope',11: 'ca',12: 'thal',13:'target'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.iloc[:,:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model= RandomForestClassifier(n_estimators=100,random_state=0)\nmodel.fit(x,y)\npd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(df, hue=\"target\", height=5) \\\n   .map(plt.scatter, \"age\", \"thalach\") \\\n   .add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(df, hue=\"target\", height=5) \\\n   .map(plt.scatter, \"age\", \"oldpeak\") \\\n   .add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(df, hue=\"target\", height=3) \\\n   .map(plt.scatter, \"thal\", \"ca\") \\\n   .add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(df, hue=\"target\", height=5) \\\n   .map(plt.scatter, \"oldpeak\", \"thalach\") \\\n   .add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(df, hue=\"target\", height=3) \\\n   .map(plt.scatter, \"restecg\", \"ca\") \\\n   .add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number=[0,1,2]\nfor col in df.itertuples():\n\n    if col.cp in number:\n        df['cp'].replace(to_replace=col.cp, value=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing Accuracy when top 8 features are used\ndf_top8 = df.loc[:,['cp','oldpeak','thal','ca','thalach','age','chol','trestbps','exang']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(df_top8,y,test_size=0.25,random_state=0)\nclf = AdaBoostClassifier()\nclf.fit(x_train,y_train)\nprediction = clf.predict(x_test)\naccuracy = accuracy_score(prediction,y_test)\n\n# Confusion matrix is the amount of misclassfied in each group\n# The diagonal is the correctly classified elements\n# When target = 0, 26 are correctly classified and 6 elements are misclassfied\n# When target = 1, 37 are correctly classified and 6 elements are misclassified\ncm = confusion_matrix(prediction,y_test)\n\n\nprfs = precision_recall_fscore_support(prediction,y_test)\nprint('Accuracy: ',accuracy)\nprint('\\n')\nprint('Confusion Matrix: \\n',cm)\nprint('\\n')\nprint('Precision: ', prfs[0])\nprint('Recall:    ', prfs[1])\nprint('Fscore:    ', prfs[2])\nprint('Support:   ', prfs[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing whether Standardization could improve accuracy\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.iloc[:,:-1]\nx_std = StandardScaler().fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x_std,y,test_size=0.25,random_state=0)\nclf = AdaBoostClassifier()\nclf.fit(x_train,y_train)\nprediction = clf.predict(x_test)\naccuracy = accuracy_score(prediction,y_test)\ncm = confusion_matrix(prediction,y_test)\nprfs = precision_recall_fscore_support(prediction,y_test)\nprint('Accuracy: ',accuracy)\nprint('\\n')\nprint('Confusion Matrix: ',cm)\nprint('\\n')\nprint('Precision: ', prfs[0])\nprint('Recall:    ', prfs[1])\nprint('Fscore:    ', prfs[2])\nprint('Support:   ', prfs[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing accuracy with different test sizes\ntestSize = [0.5,0.4,0.3,0.25,0.2,0.15,0.1]\n\nacc = []\nfor i in testSize:\n    x_train,x_test,y_train,y_test = train_test_split(x_std,y,test_size=i)\n    clf = AdaBoostClassifier()\n    clf.fit(x_train,y_train)\n    prediction=clf.predict(x_test)\n    acc.append(accuracy_score(prediction,y_test))\n\nmodels_dataframe=pd.DataFrame(acc,index=testSize)   \nmodels_dataframe\n\n#^ Allows us to see that the training size in the range of 70% to  75% gives the best result, otherwise we get underfitting or overfitting issues.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}