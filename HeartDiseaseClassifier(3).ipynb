{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_blobs\n\n# Create our dataset\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.0)\n\n# Create scatter plot\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier().fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General function to visualize a model with X and y parameters\n# We are going to keep using this function for later purposes\ndef visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n    # Define size of the plot\n    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n\n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n               clim=(y.min(), y.max()), zorder=3)\n    \n    # Set axis parameters\n    ax.axis('tight')\n    ax.axis('off')\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator. Assuming we passed in the correct model\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the fn above \nvisualize_classifier(DecisionTreeClassifier(), X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom ipywidgets import interact\n\n## This code block will be available as a copy-paste\ndef visualize_tree(estimator, X, y, boundaries=True,\n                   xlim=None, ylim=None, ax=None):\n    ax = ax or plt.gca()\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis',\n               clim=(y.min(), y.max()), zorder=3)\n    # Set ax options\n    ax.axis('tight')\n    ax.axis('off')\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    estimator.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    n_classes = len(np.unique(y))\n    Z = Z.reshape(xx.shape)\n    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap='viridis', clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n    \n    # Plot the decision boundaries\n    def plot_boundaries(i, xlim, ylim):\n        if i >= 0:\n            tree = estimator.tree_\n        \n            if tree.feature[i] == 0:\n                ax.plot([tree.threshold[i], tree.threshold[i]], ylim, '-k', zorder=2)\n                plot_boundaries(tree.children_left[i],\n                                [xlim[0], tree.threshold[i]], ylim)\n                plot_boundaries(tree.children_right[i],\n                                [tree.threshold[i], xlim[1]], ylim)\n        \n            elif tree.feature[i] == 1:\n                ax.plot(xlim, [tree.threshold[i], tree.threshold[i]], '-k', zorder=2)\n                plot_boundaries(tree.children_left[i], xlim,\n                                [ylim[0], tree.threshold[i]])\n                plot_boundaries(tree.children_right[i], xlim,\n                                [tree.threshold[i], ylim[1]])\n            \n    if boundaries:\n        plot_boundaries(0, xlim, ylim)\n\n\ndef plot_tree_interactive(X, y):\n    def interactive_tree(depth=5):\n        clf = DecisionTreeClassifier(max_depth=depth, random_state=0)\n        visualize_tree(clf, X, y)\n\n    return interact(interactive_tree, depth=[1, 5])\n\n\ndef randomized_tree_interactive(X, y):\n    N = int(0.75 * X.shape[0])\n    \n    xlim = (X[:, 0].min(), X[:, 0].max())\n    ylim = (X[:, 1].min(), X[:, 1].max())\n    \n    def fit_randomized_tree(random_state=0):\n        clf = DecisionTreeClassifier(max_depth=15)\n        i = np.arange(len(y))\n        rng = np.random.RandomState(random_state)\n        rng.shuffle(i)\n        visualize_tree(clf, X[i[:N]], y[i[:N]], boundaries=False,\n                       xlim=xlim, ylim=ylim)\n    \n    interact(fit_randomized_tree, random_state=[0, 100]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\nvisualize_tree(model, X[::2], y[::2], boundaries=False, ax=ax[0])\nvisualize_tree(model, X[1::2], y[1::2], boundaries=False, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_blobs\n\n        \nfig, ax = plt.subplots(1, 2, figsize=(16, 10))\nfig.subplots_adjust(left=0.02, right=0.98, wspace=0.1)\n\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.0)\n\nfor axi, depth in zip(ax, range(75, 77)):\n    model = DecisionTreeClassifier(max_depth=depth)\n    visualize_tree(model, X, y, ax=axi)\n    axi.set_title('depth = {0}'.format(depth))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier(max_depth=15)\nvisualize_tree(model, X, y, ax=axi)\naxi.set_title('depth = {0}'.format(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\ntree = DecisionTreeClassifier()\nbag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n                        random_state=1)\n\nbag.fit(X, y)\nvisualize_classifier(bag, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Send the random forest classifier to be fitted and plotted\nmodel = RandomForestClassifier(n_estimators=100, random_state=0)\nvisualize_classifier(model, X, y);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}